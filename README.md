# VAE generative demo

This repository contains the demo presented at AI Tinkerers (August 2025)

The goal is to illustrate the essence of generative AI: mimicking a data distribution from a limited set of samples

## Workflow

The complete pipeline consists of

* Generate a synthetic dataset of geometrical shapes of N spikes, where first one is a triangle and last one a circle
* Train a CNN to demonstrate data learnability
* Train a VAE to learn the data distribution from the images
* Plot by different projection methods (PCA / TSN-E / UMAP) the data distribution
* Using the VAE we navigate over the latent space for each class to see how the data is transformed

## Installation

In order to run the scripts, you'll need to install dependencies

```
pip install -r requirements.txt
```

After that, you'll need to install the main package

```
pip install .
```

## Usage

For each step described above we have a script to handle it, also outputting different artifacts

### Dataset Generation

For generating the dataset you can run the script

```
python -m src.data.generate_shapes
  --num-classes 5 \
  --images-per-class 3000 \
  --img-size 64 \
  --fill \
  --out outputs/datasets/5_classes.npz
```

Where
* --num-classes is the amount of geometrical shapes to generate (in this case triangle, square, pentagon, hexagon and circle) (first and last are always triangle and circle)
* --images-per-class is the amount of samples to generate per sample
* --img-size is the size of the images to generate (in this case 64x64)
* --fill is a boolean to define if the images are filled or just shaped
* --out is the output folder, to be used in the next scripts

Static output will be `outputs/dataset` containing the dataset and `outputs/figures/dataset` containing a grid for each sample generated

### CNN training

For training a CNN you can run the script

```
python -m src.train_cnn \
  --data outputs/datasets/5_classes.npz \
  --epochs 10  \
  --batch 128 \
  --outdir outputs/checkpoints/cnn_5_classes
```

Where
* --data is the dataset path generated in the step above
* --epochs is the amount of epochs to train the model
* --batch is the batch size
* --outdir is the output dir for the model weights

Static output will be `outputs/dataset/cnn_5_classes` containing the best model


### VAE training

For training a VAE you can run the script

```
python -m src.train_cvae \
  --data outputs/datasets/5_classes.npz \
  --epochs 15 \
  --batch 256 \
  --z-dim 2 \
  --beta 1.0 \
  --kl-anneal 0.3 \
  --outdir outputs/checkpoints/cvae_5_classes
```

Where:
* --data is the dataset path generated in the step above
* --epochs is the amount of epochs to train the model
* --batch is the batch size
* --z-dim is the latent space z dim, 2 is enough for this demo
* --beta 1.0 is the beta factor of the VAE
* --kl-anneal is the kl-anneal factor
* --outdir is the output dir for the model weights

Static output will be `outputs/dataset/cvae_5_classes` containing the best model


### Embedding Plotting

For plotting the dataset projection you can run the script

```
python -m src.viz.embed_plot \
  --features outputs/checkpoints/cnn_5_classes/features_val.pt \
  --methods pca tsne umap \
  --data outputs/datasets/5_classes.npz \
  --outdir outputs/figures/embeddings
```

Where:

* --features is the dir where the features are saved
* --methods are the methods available to project the data
* --data is the dataset path generated in the step above
* --outdir is the output dir for the model weights

Static output will be `outputs/figures/embeddings` where you'll check the data projection by each method

### Latent Space Walking

For making a latent space walking you can run the script by 2 methods:

1. Sequential classes (0 to 9) into a GIF:
```
python -m src.viz.latent_walk \
  --ckpt outputs/checkpoints/cvae_5_classes/cvae_best.pt \
  --data outputs/datasets/5_classes.npz \
  --mode sequence \
  --steps 24 \
  --fps 10 \
  --outdir outputs/figures/latent
```

2. Random order across all classes into a GIF:
```
python -m src.viz.latent_walk \
  --ckpt outputs/checkpoints/cvae_5_classes/cvae_best.pt \
  --data outputs/datasets/5_classes.npz \
  --mode randomseq \
  --steps 24 \
  --fps 10 \
  --outdir outputs/figures/latent
```

Where:
* --ckpt is the checkpoint path for the conditional vae trained
* --data is the dataset path generated in the step above
* --mode is the mode of walking the dataset (sequence for linear walking, randomseq for non linear walking)
* --steps is how many steps will be performed
* --fps is the fps generated by the gif
* --outdir is the output dir for the latent space walking

Static output will be `outputs/figures/latent/sequence_order.gif` or `outputs/figures/latent/sequence_random.gif` where you'll check the latent space walking

## License
This project is licensed under the GNU General Public License v3.0 (GPL-3.0)

```
Copyright (C) 2025 Thefrancho

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.
```