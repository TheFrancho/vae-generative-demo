# VAE generative demo

![Sequence Order 5 edges](./statics/sequence_order.gif) 
![Random Order 5 edges](./statics/sequence_random.gif)


This repository contains the code presented at Pereira Tech Talks Lighting Talk (November 2025) and AI Tinkerers Pereira demo showcase (August 2025)

The goal is to illustrate the essence of generative AI: mimicking a data distribution from a limited set of samples

## Workflow

The complete pipeline consists of

* Generate a synthetic dataset of geometrical shapes of N spikes, where first one is a triangle and last one a circle
* Train a CNN to demonstrate data learnability
* Train a VAE to learn the data distribution from the images
* Plot by different projection methods (PCA / TSN-E / UMAP) the data distribution
* Using the VAE we navigate over the latent space for each class to see how the data is transformed

## Installation

In order to run the scripts, you'll need to install dependencies

```
pip install -r requirements.txt
```

After that, you'll need to install the main package

```
pip install .
```

## Usage

For each step described above we have a script to handle it, also outputting different artifacts

### Dataset Generation

For generating the dataset you can run the script

```
python -m src.data.generate_shapes \
  --num-classes 5 \
  --images-per-class 3000 \
  --img-size 64 \
  --fill \
  --out outputs/datasets/5_classes.npz
```

Where
* --num-classes is the amount of geometrical shapes to generate (in this case triangle, square, pentagon, hexagon and circle) (first and last are always triangle and circle)
* --images-per-class is the amount of samples to generate per sample
* --img-size is the size of the images to generate (in this case 64x64)
* --fill is a boolean to define if the images are filled or just shaped
* --out is the output folder, to be used in the next scripts

Static output will be `outputs/dataset` containing the dataset and `outputs/figures/dataset` containing a grid for each sample generated

### CNN training

For training a CNN you can run the script

```
python -m src.train_cnn \
  --data outputs/datasets/5_classes.npz \
  --epochs 10  \
  --batch 128 \
  --outdir outputs/checkpoints/cnn_5_classes
```

Where
* --data is the dataset path generated in the step above
* --epochs is the amount of epochs to train the model
* --batch is the batch size
* --outdir is the output dir for the model weights

Static output will be `outputs/dataset/cnn_5_classes` containing the best model


### VAE training

For training a VAE you can run the script

```
python -m src.train_cvae \
  --data outputs/datasets/5_classes.npz \
  --epochs 15 \
  --batch 256 \
  --z-dim 2 \
  --beta 1.0 \
  --kl-anneal 0.3 \
  --outdir outputs/checkpoints/cvae_5_classes
```

Where:
* --data is the dataset path generated in the step above
* --epochs is the amount of epochs to train the model
* --batch is the batch size
* --z-dim is the latent space z dim, 2 is enough for this demo
* --beta 1.0 is the beta factor of the VAE
* --kl-anneal is the kl-anneal factor
* --outdir is the output dir for the model weights

Static output will be `outputs/dataset/cvae_5_classes` containing the best model


### Embedding Plotting

For plotting the dataset projection you can run the script

```
python -m src.viz.embed_plot \
  --features outputs/checkpoints/cnn_5_classes/features_val.pt \
  --methods pca tsne umap \
  --data outputs/datasets/5_classes.npz \
  --outdir outputs/figures/embeddings
```

Where:

* --features is the dir where the features are saved
* --methods are the methods available to project the data
* --data is the dataset path generated in the step above
* --outdir is the output dir for the model weights

Static output will be `outputs/figures/embeddings` where you'll check the data projection by each method

### Latent Space Walking

For making a latent space walking you can run the script by 2 methods:

1. Sequential classes (0 to 9) into a GIF:
```
python -m src.viz.latent_walk \
  --ckpt outputs/checkpoints/cvae_5_classes/cvae_best.pt \
  --data outputs/datasets/5_classes.npz \
  --mode sequence \
  --steps 24 \
  --fps 10 \
  --outdir outputs/figures/latent
```

2. Random order across all classes into a GIF:
```
python -m src.viz.latent_walk \
  --ckpt outputs/checkpoints/cvae_5_classes/cvae_best.pt \
  --data outputs/datasets/5_classes.npz \
  --mode randomseq \
  --steps 24 \
  --fps 10 \
  --outdir outputs/figures/latent
```

Where:
* --ckpt is the checkpoint path for the conditional vae trained
* --data is the dataset path generated in the step above
* --mode is the mode of walking the dataset (sequence for linear walking, randomseq for non linear walking)
* --steps is how many steps will be performed
* --fps is the fps generated by the gif
* --outdir is the output dir for the latent space walking

Static output will be `outputs/figures/latent/sequence_order.gif` or `outputs/figures/latent/sequence_random.gif` where you'll check the latent space walking

## License
This project is licensed under the GNU General Public License v3.0 (GPL-3.0)

```
Copyright (C) 2025 Thefrancho

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.
```